{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:07:43.351436Z",
     "start_time": "2026-01-17T01:07:43.342920Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install ragas",
   "id": "76e084eed36fbeab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-17T01:08:11.984692Z",
     "start_time": "2026-01-17T01:07:43.960766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "from langchain_classic.vectorstores import Chroma\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_classic.text_splitter import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.metrics.collections import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "import google.genai as genai\n",
    "from sympy.physics.units import temperature\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ana\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ana\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\instructor\\providers\\gemini\\client.py:5: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai  # type: ignore[import-not-found]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Preparing test data",
   "id": "af874ddb32896afa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:08:12.060774Z",
     "start_time": "2026-01-17T01:08:12.047151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "knowledge_docs = [\n",
    "    \"\"\"Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
    "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
    "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes. A IA pode ser classificada\n",
    "    em IA fraca (espec√≠fica para tarefas) e IA forte (intelig√™ncia geral).\"\"\",\n",
    "\n",
    "    \"\"\"Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
    "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
    "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes. Existem tr√™s tipos principais:\n",
    "    aprendizado supervisionado, n√£o supervisionado e por refor√ßo.\"\"\",\n",
    "\n",
    "    \"\"\"Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
    "    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\n",
    "    processamento de linguagem natural e reconhecimento de voz. As redes neurais profundas\n",
    "    podem ter centenas de camadas e milh√µes de par√¢metros.\"\"\",\n",
    "\n",
    "    \"\"\"RAG (Retrieval-Augmented Generation) √© uma t√©cnica que combina recupera√ß√£o de informa√ß√µes\n",
    "    com gera√ß√£o de texto. Permite que modelos de linguagem acessem conhecimento externo\n",
    "    para gerar respostas mais precisas e atualizadas. O processo envolve buscar documentos\n",
    "    relevantes e usar essas informa√ß√µes para gerar a resposta final.\"\"\",\n",
    "\n",
    "    \"\"\"Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google,\n",
    "    capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de\n",
    "    racioc√≠nio e compreens√£o contextual. O Gemini vem em diferentes vers√µes:\n",
    "    Nano, Pro e Ultra, cada uma otimizada para diferentes casos de uso.\"\"\",\n",
    "\n",
    "    \"\"\"LangChain √© um framework para desenvolvimento de aplica√ß√µes com modelos de linguagem.\n",
    "    Facilita a cria√ß√£o de cadeias complexas, gerenciamento de mem√≥ria e integra√ß√£o\n",
    "    com diferentes fontes de dados. Oferece componentes modulares para construir\n",
    "    aplica√ß√µes robustas de IA conversacional.\"\"\"\n",
    "]\n",
    "\n",
    "# Convers√£o para objetos Document\n",
    "docs = [Document(page_content=doc) for doc in knowledge_docs]\n",
    "\n",
    "print(f\"{len(docs)} knowledge documents created.\")"
   ],
   "id": "b9aaa4ef0fe5376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 knowledge documents created.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:08:12.138335Z",
     "start_time": "2026-01-17T01:08:12.130284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cria√ß√£o de dataset de teste para avalia√ß√£o RAGAS\n",
    "test_data = {\n",
    "    'question': [\n",
    "        \"O que √© Intelig√™ncia Artificial?\",\n",
    "        \"Como funciona o Machine Learning?\",\n",
    "        \"Quais s√£o as aplica√ß√µes do Deep Learning?\",\n",
    "        \"O que √© RAG e como funciona?\",\n",
    "        \"Quais s√£o as caracter√≠sticas do Google Gemini?\"\n",
    "    ],\n",
    "    'ground_truth': [\n",
    "        \"Intelig√™ncia Artificial √© um campo da ci√™ncia da computa√ß√£o focado na cria√ß√£o de sistemas que realizam tarefas que requerem intelig√™ncia humana, incluindo aprendizado, racioc√≠nio e tomada de decis√µes.\",\n",
    "        \"Machine Learning permite que computadores aprendam automaticamente atrav√©s da experi√™ncia, identificando padr√µes em dados para fazer previs√µes, sem programa√ß√£o expl√≠cita.\",\n",
    "        \"Deep Learning √© eficaz para reconhecimento de imagem, processamento de linguagem natural e reconhecimento de voz, usando redes neurais com m√∫ltiplas camadas.\",\n",
    "        \"RAG combina recupera√ß√£o de informa√ß√µes com gera√ß√£o de texto, permitindo que modelos acessem conhecimento externo para respostas mais precisas.\",\n",
    "        \"Google Gemini √© um modelo multimodal que processa texto, imagens e c√≥digo, oferecendo capacidades avan√ßadas de racioc√≠nio em vers√µes Nano, Pro e Ultra.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Dataset created!\")\n",
    "print(f\"üìä {len(test_data['question'])} test questions created.\")"
   ],
   "id": "c24c53f4adb6419c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset created!\n",
      "üìä 5 test questions created.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. RAG system creation",
   "id": "74e45b69836f70fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:08:19.060612Z",
     "start_time": "2026-01-17T01:08:12.163318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=\"./chroma_db_evaluation\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3, convert_system_message_to_human=True)\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=3, return_messages=True, output_key=\"answer\")\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\":3}),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")"
   ],
   "id": "e345fa33e5ecaea1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ana\\AppData\\Local\\Temp\\ipykernel_14516\\2210280011.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=3, return_messages=True, output_key=\"answer\")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the Cascade Effect\n",
    "In **RAG (Retrieval-Augmented Generation)** systems, every component‚Äîfrom retrieval to response generation‚Äîcan be thought of as part of a chain where an error in one stage can decisively impact the rest. This phenomenon, known as the **cascade effect**, means that an initial failure, such as retrieving irrelevant or incomplete documents, can lead to incorrect or even confusing responses, compromising the reliability of the entire system.\n",
    "\n",
    "When the input for the language model is poorly grounded, even a robust LLM tends to produce responses that do not align with the expected context. Thus, the system exemplifies the **\"garbage in, garbage out\"** principle, reinforcing the need for a meticulous evaluation of every stage.\n",
    "\n",
    "\n",
    "\n",
    "### Error Propagation: From Retrieval to Generation\n",
    "Imagine a scenario where the retrieval layer fails to fetch essential documents for the user's query. The lack of relevant information results in a fragile context base. When this context is passed to the generation layer, the model may produce a response with low factuality, as it lacks the correct data to validate its textual output.\n",
    "\n",
    "This interdependence makes the evaluation of RAG systems more complex; it is necessary not only to measure the quality of each component in isolation but also how they integrate and contribute to the system's overall performance.\n",
    "\n",
    "### Example of Error Propagation\n",
    "Consider the following illustrative pseudocode:\n",
    "\n",
    "```python\n",
    "# Simulation of a simplified chain in a RAG system\n",
    "\n",
    "# Step 1: Document Retrieval\n",
    "documents = retrieve_documents(query)\n",
    "\n",
    "# Simplified document quality validation\n",
    "if not documents or relevant_documents_count(documents) < 0.5:\n",
    "    log_error('Retrieval failed: insufficient context')\n",
    "    context = 'generic content'\n",
    "else:\n",
    "    context = combine_documents(documents)\n",
    "\n",
    "# Step 2: Response generation based on context\n",
    "response = model_generate_response(query, context)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "In this example, the check during the retrieval stage allows for the identification of a potential failure before the error propagates to the generation stage. Although simplified, it illustrates the impact that a compromised stage can have on the entire processing chain.\n",
    "\n",
    "### Mitigation Strategies\n",
    "To minimize the impact of the cascade effect, it is fundamental to adopt monitoring and validation mechanisms at each stage. Possible strategies include:\n",
    "\n",
    "* **Retrieval Redundancy:** Using multiple sources or search methods to ensure the context is complete, reducing the chance of isolated failures compromising the system.\n",
    "* **Intermediate Validation:** Implementing checkpoints that validate context quality before proceeding to response generation. This can include using automated metrics to detect discrepancies and inconsistencies.\n",
    "* **Feedback and Dynamic Adjustments:** Creating a feedback loop that allows the system to self-evaluate and adjust parameters based on both quantitative metrics and qualitative evaluations from practical testing.\n",
    "\n",
    "\n",
    "\n",
    "These approaches contribute to a more resilient system, capable of detecting and correcting errors before they turn into cascading failures, thus ensuring more accurate and consistent responses for users."
   ],
   "id": "db08e4363c9655f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Collecting data for evaluation",
   "id": "5806b101923f7196"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:10:15.042888Z",
     "start_time": "2026-01-17T01:08:19.092558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_rag_collect_data(questions):\n",
    "    \"\"\"Runs the RAG chain on the provided questions and returns the results.\"\"\"\n",
    "\n",
    "    results = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"ground_truth\": [],\n",
    "    }\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}...\")\n",
    "        try:\n",
    "            result = rag_chain.invoke({\"question\": question})\n",
    "\n",
    "            contexts = [doc.page_content for doc in result[\"source_documents\"]]\n",
    "\n",
    "            results[\"question\"].append(question)\n",
    "            results[\"answer\"].append(result[\"answer\"])\n",
    "            results[\"contexts\"].append(contexts)\n",
    "            results[\"ground_truth\"].append(test_data[\"ground_truth\"][i]) # it was better pulling it from the input variables\n",
    "\n",
    "            print(f\"Generated answer: {result['answer'][:100]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Starting data collection for evaluation...\")\n",
    "evaluation_data = run_rag_collect_data(test_data[\"question\"])\n",
    "\n",
    "print(f\"\\nSuccessfully collected {len(evaluation_data['question'])} questions for evaluation.\")"
   ],
   "id": "e6d91a5271ea3cd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection for evaluation...\n",
      "Processing question 1/5...\n",
      "Error processing question 1: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 4.660751217s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}\n",
      "Processing question 2/5...\n",
      "Error processing question 2: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 27.165302859s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}\n",
      "Processing question 3/5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m     31\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting data collection for evaluation...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m evaluation_data = \u001B[43mrun_rag_collect_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mSuccessfully collected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(evaluation_data[\u001B[33m'\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m questions for evaluation.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mrun_rag_collect_data\u001B[39m\u001B[34m(questions)\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mProcessing question \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(questions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     result = \u001B[43mrag_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m     contexts = [doc.page_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m result[\u001B[33m\"\u001B[39m\u001B[33msource_documents\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m     18\u001B[39m     results[\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m].append(question)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m    172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    173\u001B[39m         inputs,\n\u001B[32m    174\u001B[39m         outputs,\n\u001B[32m    175\u001B[39m         return_only_outputs,\n\u001B[32m    176\u001B[39m     )\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\conversational_retrieval\\base.py:177\u001B[39m, in \u001B[36mBaseConversationalRetrievalChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    175\u001B[39m         new_inputs[\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m] = new_question\n\u001B[32m    176\u001B[39m     new_inputs[\u001B[33m\"\u001B[39m\u001B[33mchat_history\u001B[39m\u001B[33m\"\u001B[39m] = chat_history_str\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcombine_docs_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_documents\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_run_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    182\u001B[39m     output[\u001B[38;5;28mself\u001B[39m.output_key] = answer\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_source_documents:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:206\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    204\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    205\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:637\u001B[39m, in \u001B[36mChain.run\u001B[39m\u001B[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[39m\n\u001B[32m    632\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[32m0\u001B[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[32m    633\u001B[39m         _output_key\n\u001B[32m    634\u001B[39m     ]\n\u001B[32m    636\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[32m--> \u001B[39m\u001B[32m637\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[32m    638\u001B[39m         _output_key\n\u001B[32m    639\u001B[39m     ]\n\u001B[32m    641\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[32m    642\u001B[39m     msg = (\n\u001B[32m    643\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    644\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m but none were provided.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    645\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:206\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    204\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    205\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:413\u001B[39m, in \u001B[36mChain.__call__\u001B[39m\u001B[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[32m    381\u001B[39m \n\u001B[32m    382\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    404\u001B[39m \u001B[33;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[32m    405\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    406\u001B[39m config = {\n\u001B[32m    407\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks,\n\u001B[32m    408\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m: tags,\n\u001B[32m    409\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m    410\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m: run_name,\n\u001B[32m    411\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRunnableConfig\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    416\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    418\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m    172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    173\u001B[39m         inputs,\n\u001B[32m    174\u001B[39m         outputs,\n\u001B[32m    175\u001B[39m         return_only_outputs,\n\u001B[32m    176\u001B[39m     )\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\combine_documents\\base.py:141\u001B[39m, in \u001B[36mBaseCombineDocumentsChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[32m    140\u001B[39m other_keys = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs.items() \u001B[38;5;28;01mif\u001B[39;00m k != \u001B[38;5;28mself\u001B[39m.input_key}\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m output, extra_return_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_run_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mother_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    146\u001B[39m extra_return_dict[\u001B[38;5;28mself\u001B[39m.output_key] = output\n\u001B[32m    147\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\combine_documents\\stuff.py:266\u001B[39m, in \u001B[36mStuffDocumentsChain.combine_docs\u001B[39m\u001B[34m(self, docs, callbacks, **kwargs)\u001B[39m\n\u001B[32m    264\u001B[39m inputs = \u001B[38;5;28mself\u001B[39m._get_inputs(docs, **kwargs)\n\u001B[32m    265\u001B[39m \u001B[38;5;66;03m# Call predict on the LLM.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m266\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\llm.py:315\u001B[39m, in \u001B[36mLLMChain.predict\u001B[39m\u001B[34m(self, callbacks, **kwargs)\u001B[39m\n\u001B[32m    300\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks = \u001B[38;5;28;01mNone\u001B[39;00m, **kwargs: Any) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    301\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[32m    302\u001B[39m \n\u001B[32m    303\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    313\u001B[39m \u001B[33;03m        ```\u001B[39;00m\n\u001B[32m    314\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m315\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m.output_key]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:206\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    204\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    205\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:413\u001B[39m, in \u001B[36mChain.__call__\u001B[39m\u001B[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[32m    381\u001B[39m \n\u001B[32m    382\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    404\u001B[39m \u001B[33;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[32m    405\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    406\u001B[39m config = {\n\u001B[32m    407\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks,\n\u001B[32m    408\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m: tags,\n\u001B[32m    409\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m    410\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m: run_name,\n\u001B[32m    411\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRunnableConfig\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    416\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    418\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m    172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    173\u001B[39m         inputs,\n\u001B[32m    174\u001B[39m         outputs,\n\u001B[32m    175\u001B[39m         return_only_outputs,\n\u001B[32m    176\u001B[39m     )\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\llm.py:117\u001B[39m, in \u001B[36mLLMChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_call\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    114\u001B[39m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m    115\u001B[39m     run_manager: CallbackManagerForChainRun | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    116\u001B[39m ) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_outputs(response)[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\llm.py:129\u001B[39m, in \u001B[36mLLMChain.generate\u001B[39m\u001B[34m(self, input_list, run_manager)\u001B[39m\n\u001B[32m    127\u001B[39m callbacks = run_manager.get_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.llm, BaseLanguageModel):\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m results = \u001B[38;5;28mself\u001B[39m.llm.bind(stop=stop, **\u001B[38;5;28mself\u001B[39m.llm_kwargs).batch(\n\u001B[32m    136\u001B[39m     cast(\u001B[33m\"\u001B[39m\u001B[33mlist\u001B[39m\u001B[33m\"\u001B[39m, prompts),\n\u001B[32m    137\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks},\n\u001B[32m    138\u001B[39m )\n\u001B[32m    139\u001B[39m generations: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[Generation]] = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1112\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1113\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1114\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1118\u001B[39m     **kwargs: Any,\n\u001B[32m   1119\u001B[39m ) -> LLMResult:\n\u001B[32m   1120\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    930\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m931\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    937\u001B[39m         )\n\u001B[32m    938\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    939\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1225\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1223\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1224\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1225\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1226\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1227\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1228\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1229\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3047\u001B[39m, in \u001B[36mChatGoogleGenerativeAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001B[39m\n\u001B[32m   3034\u001B[39m request = \u001B[38;5;28mself\u001B[39m._prepare_request(\n\u001B[32m   3035\u001B[39m     messages,\n\u001B[32m   3036\u001B[39m     stop=stop,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3044\u001B[39m     **kwargs,\n\u001B[32m   3045\u001B[39m )\n\u001B[32m   3046\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3047\u001B[39m     response: GenerateContentResponse = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3048\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3049\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3050\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ClientError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3051\u001B[39m     _handle_client_error(e, request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5215\u001B[39m, in \u001B[36mModels.generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   5213\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m remaining_remote_calls_afc > \u001B[32m0\u001B[39m:\n\u001B[32m   5214\u001B[39m   i += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m5215\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5216\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparsed_config\u001B[49m\n\u001B[32m   5217\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5219\u001B[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001B[32m   5220\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m function_map:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\google\\genai\\models.py:3997\u001B[39m, in \u001B[36mModels._generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   3994\u001B[39m request_dict = _common.convert_to_dict(request_dict)\n\u001B[32m   3995\u001B[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001B[32m-> \u001B[39m\u001B[32m3997\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_api_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3998\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpost\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_options\u001B[49m\n\u001B[32m   3999\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4001\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[32m   4002\u001B[39m     config, \u001B[33m'\u001B[39m\u001B[33mshould_return_http_response\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4003\u001B[39m ):\n\u001B[32m   4004\u001B[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1375\u001B[39m, in \u001B[36mBaseApiClient.request\u001B[39m\u001B[34m(self, http_method, path, request_dict, http_options)\u001B[39m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrequest\u001B[39m(\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1367\u001B[39m     http_method: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1370\u001B[39m     http_options: Optional[HttpOptionsOrDict] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1371\u001B[39m ) -> SdkHttpResponse:\n\u001B[32m   1372\u001B[39m   http_request = \u001B[38;5;28mself\u001B[39m._build_request(\n\u001B[32m   1373\u001B[39m       http_method, path, request_dict, http_options\n\u001B[32m   1374\u001B[39m   )\n\u001B[32m-> \u001B[39m\u001B[32m1375\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   1376\u001B[39m   response_body = (\n\u001B[32m   1377\u001B[39m       response.response_stream[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m response.response_stream \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m   1378\u001B[39m   )\n\u001B[32m   1379\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1209\u001B[39m, in \u001B[36mBaseApiClient._request\u001B[39m\u001B[34m(self, http_request, http_options, stream)\u001B[39m\n\u001B[32m   1207\u001B[39m     retry_kwargs = retry_args(parameter_model.retry_options)\n\u001B[32m   1208\u001B[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1209\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_once\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[no-any-return]\u001B[39;00m\n\u001B[32m   1211\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retry(\u001B[38;5;28mself\u001B[39m._request_once, http_request, stream)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:487\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    485\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoSleep):\n\u001B[32m    486\u001B[39m     retry_state.prepare_for_next_attempt()\n\u001B[32m--> \u001B[39m\u001B[32m487\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    489\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m do\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\tenacity\\nap.py:31\u001B[39m, in \u001B[36msleep\u001B[39m\u001B[34m(seconds)\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msleep\u001B[39m(seconds: \u001B[38;5;28mfloat\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     26\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     27\u001B[39m \u001B[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001B[39;00m\n\u001B[32m     28\u001B[39m \n\u001B[32m     29\u001B[39m \u001B[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseconds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. RAGAS evaluation",
   "id": "1daf69ac74adb8ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "print(\"Starting RAGAS evaluation...\")\n",
    "ragas_dataset = Dataset.from_dict(evaluation_data)\n",
    "\n",
    "ragas_metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "print(\"\\nRAGAS metrics ready:\")\n",
    "for metric in ragas_metrics:\n",
    "    print(f\"-   {metric.name}\")"
   ],
   "id": "8fa1570b3cf7b4e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nEvaluating RAGAS metrics...\")\n",
    "\n",
    "# config = RunConfig( # para evitar timeout\n",
    "#     timeout=180,      # Aumenta para 3 minutos por requisi√ß√£o\n",
    "#     max_retries=10,   # Tenta de novo se falhar\n",
    "#     max_workers=2,    # Reduz o paralelismo (o padr√£o costuma ser 16)\n",
    "#     max_wait=60       # Espera at√© 60s entre as tentativas\n",
    "# )\n",
    "\n",
    "ragas_results = evaluate(\n",
    "    dataset=ragas_dataset,\n",
    "    metrics=ragas_metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    #run_config=config\n",
    ")\n",
    "print(\"\\nRAGAS evaluation results:\")\n",
    "print(f\"Faithfulness: {ragas_results['faithfulness'][0]:.4f}\")\n",
    "print(f\"Answer relevancy: {ragas_results['answer_relevancy'][0]:.4f}\")\n",
    "print(f\"Context precision: {ragas_results['context_precision'][0]:.4f}\")\n",
    "print(f\"Context recall: {ragas_results['context_recall'][0]:.4f}\")\n",
    "\n",
    "simulated_ragas_results = {\n",
    "    \"faithfulness\": 0.85,\n",
    "    \"answer_relevancy\": 0.78,\n",
    "    \"context_precision\": 0.82,\n",
    "    \"context_recall\": 0.75\n",
    "}\n",
    "for metric, value in simulated_ragas_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ],
   "id": "d3c19a95827f6917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Detailed metrics analysis",
   "id": "f8d4fbbd8eab83e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_metric(results):\n",
    "    \"\"\" Analyzes the provided results and prints a detailed analysis of the metrics.\"\"\"\n",
    "\n",
    "    # 1. Convert EvaluationResult to dict\n",
    "    if hasattr(results, \"to_dict\"):\n",
    "        results = results.to_dict()\n",
    "    elif hasattr(results, \"_scores_dict\"):\n",
    "        results = results._scores_dict\n",
    "    elif not isinstance(results, dict):\n",
    "        results = dict(results)\n",
    "\n",
    "    # 2. Secure helper to get score as scalar\n",
    "    def get_score(key: str, default: float = 0.0) -> float:\n",
    "        result_value = results.get(key, default)\n",
    "        if isinstance(result_value, (list, tuple)):\n",
    "            return float(result_value[0])\n",
    "        return float(result_value)\n",
    "\n",
    "    print(\"Detailed metric analysis:\")\n",
    "\n",
    "    # 3. Expected metrics\n",
    "    metrics = {\n",
    "        \"faithfulness\": \"faithfulness\",\n",
    "        \"answer_relevancy\": \"answer relevancy\",\n",
    "        \"context_relevancy\": \"context relevancy\",\n",
    "        \"context_precision\": \"context precision\",\n",
    "        \"context_recall\": \"context recall\"\n",
    "    }\n",
    "    valid_scores = []\n",
    "    for key, legible_name in metrics.items():\n",
    "        if key not in results:\n",
    "            continue\n",
    "        score = get_score(key)\n",
    "        valid_scores.append(score)\n",
    "\n",
    "        print(f\"- {legible_name}: {score:.4f}\")\n",
    "\n",
    "        if score >= 0.8:\n",
    "            print(\" Excellent\")\n",
    "        elif score >= 0.6:\n",
    "            print(\" Good\")\n",
    "        else:\n",
    "            print(\" Poor\")\n",
    "\n",
    "    # 4. Overall score\n",
    "    if valid_scores:\n",
    "        overall_score = float(np.mean(valid_scores))\n",
    "        print(f\"\\nOverall score: {overall_score:.4f}\")\n",
    "\n",
    "        if overall_score >= 0.8:\n",
    "            print(\" Excellent\")\n",
    "        elif overall_score >= 0.6:\n",
    "            print(\" Good\")\n",
    "        else:\n",
    "            print(\" Poor\")\n",
    "    else:\n",
    "        print(\"No metrics available\")"
   ],
   "id": "62e6f678ed122347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analyze_metric(ragas_results)",
   "id": "2c5a02e86448027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this lesson, we saw how to evaluate a **RAG system** using **LangSmith** and **Ragas**, covering everything from initial setup to executing metrics.\n",
    "\n",
    "Now it's your chance to review and practice the concepts from this lesson, if you haven't already. To do so:\n",
    "\n",
    "* **Install and import** the necessary libraries and tools (`LangChain`, `ChromaDB`, `Google Generative AI`, etc.).\n",
    "* **Configure access keys** and import project dependencies, including auxiliary functions.\n",
    "* **Prepare the test data** by creating knowledge documents and a Q&A dataset (questions and reference answers).\n",
    "* **Configure the RAG system**: initialize embeddings, create the vector store, instantiate the LLM, and define the memory and query chain.\n",
    "* **Implement data collection**, iterating through questions to store generated answers, contexts, and reference ground truths.\n",
    "* **Configure and execute the evaluation with Ragas**, using metrics such as **faithfulness**, **relevance**, **precision**, and **recall**, and analyze the final results.\n",
    "\n",
    "\n",
    "In this lesson, we learned:\n",
    "\n",
    "* **The importance of evaluation** in RAG systems and the challenges associated with its complexity.\n",
    "* **Using LangSmith** as an observability platform to track and monitor RAG systems.\n",
    "* **The utilization of the Ragas library** to provide specific metrics focused on faithfulness and relevance.\n",
    "* **The implementation of a complete RAG system** using Google Generative AI Embeddings, ChromaDB, and LangChain.\n",
    "* **The configuration of specific metrics**, such as Faithfulness, Answer Relevance, Context Precision, and Context Recall, for RAG evaluation.\n",
    "* **The development of a test dataset** with questions and answers to demonstrate the metrics.\n",
    "* **Data collection and storage** for evaluation, and the use of logs for monitoring.\n",
    "* **Detailed analysis and interpretation of metrics** to evaluate the overall performance of the RAG system."
   ],
   "id": "5b66437ac61310b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "274add1e685e0939",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
