{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fase 1: Ingestion pipeline",
   "id": "93cc7bf549fd41f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1 Extract",
   "id": "79226ddaea753c6f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T18:42:58.002724Z",
     "start_time": "2026-01-22T18:41:53.960019Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"../../aulas/1_LangChain_RAG\")\n",
    "docs = loader.load()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ana\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2 Transform",
   "id": "6c930e853d240ed1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:42:58.983302Z",
     "start_time": "2026-01-22T18:42:58.932387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Total created chunks: {len(chunks)}\")"
   ],
   "id": "be79eb60c2d1c41c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total created chunks: 98\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:43:04.974140Z",
     "start_time": "2026-01-22T18:42:59.017197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ],
   "id": "31e247e409b208fa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3 Load",
   "id": "72b85bb47f4d2744"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:43:13.318028Z",
     "start_time": "2026-01-22T18:43:04.997777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model\n",
    ")\n",
    "\n",
    "print(\"Database created successfully!\")"
   ],
   "id": "180bb7bbd2cce0cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fase 2: Building a advanced retrieval system",
   "id": "c1a3b089442c0af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.1 Hybrid search",
   "id": "6c468eb020460f81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:48:20.521247Z",
     "start_time": "2026-01-22T18:48:20.155849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# 1. Lexical retriever (BM25)\n",
    "bm25_rtvr = BM25Retriever.from_documents(chunks, k=5)\n",
    "\n",
    "# 2. Semantic retriever (ChromaDB)\n",
    "vctr_rtvr = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 1. Ensemble retriever (BM25)\n",
    "ensemble_rtvr = EnsembleRetriever(\n",
    "    retrievers=[bm25_rtvr, vctr_rtvr],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ],
   "id": "8e20afbd52628730",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fase 3: Robust convesational chain",
   "id": "cee3b32d35c2895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.1 Memory chain and query transformation",
   "id": "95c21ca4985e5566"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:48:23.808773Z",
     "start_time": "2026-01-22T18:48:22.447259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=ensemble_rtvr,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")"
   ],
   "id": "517bf5263ef8e020",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:49:27.646863Z",
     "start_time": "2026-01-22T18:49:22.335768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resp1 = qa_chain.invoke({\"question\": \"What's adaptative chunking?\"})\n",
    "print(resp1[\"answer\"])"
   ],
   "id": "3c1aef3961ae72a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive chunking is a technique for intelligently dividing long documents into smaller pieces (chunks) while preserving their semantic context. This process is crucial for efficient processing and retrieval, and for the quality of the responses.\n",
      "\n",
      "The strategy for chunking should be adapted to the type of data, such as continuous texts, tables, source code, or structured documents. Common methods include:\n",
      "*   **Fixed-Size Chunking:** Dividing by a fixed number of tokens or characters.\n",
      "*   **Sliding Window:** Using overlap (overlap) to maintain context between chunks.\n",
      "*   **Recursive Splitting:** Dividing based on the semantic structure of the text.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T18:50:49.319707Z",
     "start_time": "2026-01-22T18:50:43.246116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resp2 = qa_chain.invoke({\"question\": \"And what are the main strategies?\"})\n",
    "print(resp2[\"answer\"])"
   ],
   "id": "f90220744e12f2e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main strategies for adaptive chunking are:\n",
      "\n",
      "*   **Fixed-Size Chunking:** Dividing documents by a fixed number of tokens or characters.\n",
      "*   **Sliding Window:** Using overlap between chunks to maintain context.\n",
      "*   **Recursive Splitting:** Dividing based on the semantic structure of the text.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fase 4: RAGAS evaluation system",
   "id": "911ea9c46e9cffb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T19:01:00.067320Z",
     "start_time": "2026-01-22T19:00:18.997627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    _faithfulness,\n",
    "    _answer_relevancy,\n",
    "    _context_recall,\n",
    "    _context_precision\n",
    ")"
   ],
   "id": "d52a674d144e064a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ana\\Documents\\Cursos\\aluraDataScience\\.venv\\Lib\\site-packages\\instructor\\providers\\gemini\\client.py:5: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai  # type: ignore[import-not-found]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T19:03:29.209308Z",
     "start_time": "2026-01-22T19:03:29.193657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions_list = [\n",
    "    \"O que é RAG e qual problema ele soluciona?\",\n",
    "    \"Quais os componentes essenciais do RAG?\",\n",
    "    \"Qual a diferença entre busca lexical e semântica?\",\n",
    "    \"O que mede a métrica faithfulness do RAGAS?\"\n",
    "]\n",
    "rag_key_points = [\n",
    "    \"RAG (Retrieval-Augmented Generation) é uma arquitetura que combina um motor de busca para recuperar informações com um LLM para gerar respostas. Ele soluciona problemas como alucinações e conhecimento desatualizado dos LLMs.\",\n",
    "    \"Os componentes essenciais são: Embeddings, Banco de Dados Vetorial, Chunking e um Modelo de Linguagem (LLM).\",\n",
    "    \"Busca lexical (como BM25) encontra correspondências exatas de termos, enquanto a busca semântica captura o significado e o contexto, mesmo com palavras diferentes.\",\n",
    "    \"A métrica Faithfulness mede se a resposta gerada é suportada e factualmente consistente com os documentos recuperados, evitando alucinações.\"\n",
    "]"
   ],
   "id": "abb697d5cb472724",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T01:24:21.310849Z",
     "start_time": "2026-01-23T01:20:06.435467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_responses = []\n",
    "retrieved_contexts = []\n",
    "for question in questions_list:\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    generated_responses.append(result['answer'])\n",
    "    retrieved_contexts.append([doc.page_content for doc in result['source_documents']])\n",
    "\n",
    "dataset_dict = {\n",
    "    'question': questions_list,\n",
    "    'answer': generated_responses,\n",
    "    'contexts': retrieved_contexts,\n",
    "    'ground_truth': rag_key_points\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        _faithfulness,\n",
    "        _answer_relevancy,\n",
    "        _context_precision,\n",
    "        _context_recall,\n",
    "    ],\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\"),\n",
    "    embeddings=embeddings_model\n",
    ")\n",
    "\n",
    "evaluation_results_df = evaluation_result.to_pandas()\n",
    "print(\"\\nEvaluation results:\")\n",
    "display(evaluation_results_df)"
   ],
   "id": "b62e2319668a2b1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]Exception raised in Job[13]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:   6%|▋         | 1/16 [01:07<16:50, 67.38s/it]Exception raised in Job[2]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  12%|█▎        | 2/16 [01:11<07:06, 30.44s/it]Exception raised in Job[6]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  19%|█▉        | 3/16 [01:26<05:00, 23.09s/it]Exception raised in Job[12]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  25%|██▌       | 4/16 [01:43<04:09, 20.76s/it]Exception raised in Job[11]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Exception raised in Job[5]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  38%|███▊      | 6/16 [01:56<02:10, 13.08s/it]Exception raised in Job[4]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  44%|████▍     | 7/16 [02:01<01:38, 10.92s/it]Exception raised in Job[15]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  50%|█████     | 8/16 [02:01<01:03,  7.92s/it]Exception raised in Job[3]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  56%|█████▋    | 9/16 [02:05<00:47,  6.78s/it]Exception raised in Job[7]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  62%|██████▎   | 10/16 [02:06<00:31,  5.24s/it]Exception raised in Job[10]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  69%|██████▉   | 11/16 [02:16<00:32,  6.58s/it]Exception raised in Job[14]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  75%|███████▌  | 12/16 [02:26<00:30,  7.55s/it]Exception raised in Job[8]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  81%|████████▏ | 13/16 [02:42<00:29,  9.89s/it]Exception raised in Job[1]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  88%|████████▊ | 14/16 [02:50<00:18,  9.48s/it]Exception raised in Job[9]: ChatGoogleGenerativeAIError(Error calling model 'gemini-1.5-pro-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}})\n",
      "Evaluating:  94%|█████████▍| 15/16 [02:54<00:07,  7.87s/it]Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 16/16 [03:00<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                          user_input  \\\n",
       "0         O que é RAG e qual problema ele soluciona?   \n",
       "1            Quais os componentes essenciais do RAG?   \n",
       "2  Qual a diferença entre busca lexical e semântica?   \n",
       "3        O que mede a métrica faithfulness do RAGAS?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...   \n",
       "1  [alura\\nNormalização de Embeddings\\n● A normal...   \n",
       "2  [alura\\nHybrid Search (Busca Híbrida)\\nBusca s...   \n",
       "3  [alura\\nMétricas de Geração\\nO RAGAS fornece m...   \n",
       "\n",
       "                                            response  \\\n",
       "0  RAG (Retrieval-Augmented Generation) é uma sol...   \n",
       "1  Os componentes essenciais do RAG são:\\n\\n*   *...   \n",
       "2  A diferença entre busca lexical e semântica é ...   \n",
       "3  A métrica \"Faithfulness\" (Factualidade) do RAG...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  RAG (Retrieval-Augmented Generation) é uma arq...           NaN   \n",
       "1  Os componentes essenciais são: Embeddings, Ban...           NaN   \n",
       "2  Busca lexical (como BM25) encontra correspondê...           NaN   \n",
       "3  A métrica Faithfulness mede se a resposta gera...           NaN   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0               NaN                NaN             NaN  \n",
       "1               NaN                NaN             NaN  \n",
       "2               NaN                NaN             NaN  \n",
       "3               NaN                NaN             NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O que é RAG e qual problema ele soluciona?</td>\n",
       "      <td>[POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) é uma sol...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) é uma arq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quais os componentes essenciais do RAG?</td>\n",
       "      <td>[alura\\nNormalização de Embeddings\\n● A normal...</td>\n",
       "      <td>Os componentes essenciais do RAG são:\\n\\n*   *...</td>\n",
       "      <td>Os componentes essenciais são: Embeddings, Ban...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qual a diferença entre busca lexical e semântica?</td>\n",
       "      <td>[alura\\nHybrid Search (Busca Híbrida)\\nBusca s...</td>\n",
       "      <td>A diferença entre busca lexical e semântica é ...</td>\n",
       "      <td>Busca lexical (como BM25) encontra correspondê...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O que mede a métrica faithfulness do RAGAS?</td>\n",
       "      <td>[alura\\nMétricas de Geração\\nO RAGAS fornece m...</td>\n",
       "      <td>A métrica \"Faithfulness\" (Factualidade) do RAG...</td>\n",
       "      <td>A métrica Faithfulness mede se a resposta gera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fundamental Concepts\n",
    "\n",
    "**BM25**, part of the **Okapi BM** family of functions, is a classic ranking technique used to estimate the relevance of documents in information retrieval systems. It is based on the frequency with which query terms appear in each document, balancing this factor against the document's length and the rarity of the terms across the entire collection. This approach was designed to overcome the limitations of simpler models by adjusting the impact of high-frequency words and favoring documents where the terms are more significant.\n",
    "\n",
    "\n",
    "\n",
    "### How BM25 Works\n",
    "\n",
    "The algorithm uses a formula that sums the individual contributions of each query term. Generally, for each term, it considers:\n",
    "\n",
    "* **Term Frequency (TF):** How often the term appears in the document.\n",
    "* **Inverse Document Frequency (IDF):** The relative importance of the term in the collection, based on how rare it is.\n",
    "* **Normalization Factor:** An adjustment for document length.\n",
    "\n",
    "The combination of these elements allows BM25 to compute a score that reflects how well a document matches a specific query. This score is calculated taking into account that longer documents tend to have higher term counts; therefore, **normalization** is essential to avoid bias toward length.\n",
    "\n",
    "### Comparative Analysis with Other Approaches\n",
    "\n",
    "While semantic search techniques—which utilize vector representations—are on the rise, BM25 maintains significant advantages. As a **lexical method**, its interpretation is straightforward, allowing for a better understanding of how and why a document was deemed relevant. On the other hand, because it does not capture deep semantic relationships between terms, it may be less effective in situations where context or synonyms play a crucial role.\n",
    "\n",
    "\n",
    "\n",
    "In practical scenarios, combining BM25 with vector search approaches (**Hybrid Search**) has proven highly advantageous. It allows systems to leverage the best of both worlds: the robust lexical analysis of BM25 and the semantic similarity capabilities of embeddings. This integration leads to more precise and robust information retrieval, especially in diverse databases.\n",
    "\n",
    "This deep dive demonstrates that even in a world increasingly driven by AI techniques, established methods like BM25 remain relevant and effectively complement modern data retrieval strategies."
   ],
   "id": "7f0941d84027263e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this lesson, we learned:\n",
    "\n",
    "* **Using PDFs as a knowledge base** in RAG systems and chatbots.\n",
    "* **Setting up the environment** with essential libraries such as LangChain, Google Generative AI, and ChromaDB.\n",
    "* **Processing and splitting PDF documents into chunks** using the `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "\n",
    "\n",
    "* **Implementing embeddings with Google Generative AI** for high-performance vector representations.\n",
    "* **Creating a vector database with ChromaDB** and indexing both chunks and embeddings.\n",
    "* **Developing a retrieval system using hybrid search**, combining BM25 and a vector retriever.\n",
    "\n",
    "\n",
    "\n",
    "* **Configuring a conversational chain with memory** using `ConversationBufferMemory`.\n",
    "* **Utilizing the Google Gemini model** to generate deterministic responses within conversational chains."
   ],
   "id": "b1b459c6c70fe918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d55daccb62d6284c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
